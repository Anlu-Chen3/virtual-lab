{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:30:38.712768Z",
     "start_time": "2025-01-17T20:30:38.708624Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import concurrent.futures\n",
    "import json\n",
    "import re\n",
    "\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "from virtual_lab.agent import Agent\n",
    "from virtual_lab.constants import CONSISTENT_TEMPERATURE\n",
    "from virtual_lab.run_meeting import run_meeting\n",
    "from virtual_lab.utils import async_get_messages, count_tokens, get_pubmed_central_article\n",
    "\n",
    "from nanobody_constants import (\n",
    "    background_prompt,\n",
    "    nanobody_prompt,\n",
    "    discussions_phase_to_dir,\n",
    "    model,\n",
    "    finetuning_base_model,\n",
    "    immunologist,\n",
    "    machine_learning_specialist,\n",
    "    computational_biologist,\n",
    ")"
   ],
   "id": "a0472e92df8ee037",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:21:02.752553Z",
     "start_time": "2025-01-17T20:21:02.671906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "finetuning_dir = discussions_phase_to_dir[\"finetuning\"]\n",
    "papers_dir = finetuning_dir / \"papers\"\n",
    "summaries_dir = finetuning_dir / \"summaries\"\n",
    "\n",
    "papers_dir.mkdir(parents=True, exist_ok=True)\n",
    "summaries_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "client = OpenAI()\n",
    "async_client = AsyncOpenAI()\n",
    "num_concurrent = 50"
   ],
   "id": "fc28ade3791ee419",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:21:02.835739Z",
     "start_time": "2025-01-17T20:21:02.832944Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Topic to agent mapping\n",
    "topic_to_agent = {\n",
    "    \"nanobodies\": immunologist,\n",
    "    \"SARS-CoV-2 spike protein\": immunologist,\n",
    "    \"SARS-CoV-2 variants KP.3 and JN.1\": immunologist,\n",
    "    \"ESM\": machine_learning_specialist,\n",
    "    \"AlphaFold-Multimer\": computational_biologist,\n",
    "    \"Rosetta\": computational_biologist,\n",
    "}"
   ],
   "id": "164ca17b6c09fc08",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Agent fine-tuning queries\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    concurrent.futures.wait([\n",
    "        executor.submit(\n",
    "            run_meeting,\n",
    "            meeting_type=\"individual\",\n",
    "            team_member=agent,\n",
    "            agenda=f\"{background_prompt} {nanobody_prompt} You are responsible for understanding the topic \\\"{topic}\\\" in the context of designing nanobody binders for SARS-CoV-2. You need to fine-tune yourself on the relevant literature on {topic} to improve your ability to design SARS-CoV-2 nanobody binders. Please write out a series of five distinct search queries that you want to run to find relevant scientific papers on {topic}. Include both queries about {topic} generally as well as queries about how {topic} relates to designing nanobody binders for SARS-CoV-2. Please provide the queries in Python syntax as a list of double-quoted strings.\",\n",
    "            agenda_questions=(\n",
    "                f\"What are the queries that you want to perform to identify the relevant literature on {topic} (as a list of double-quoted strings in Python syntax)?\",),\n",
    "            save_dir=finetuning_dir,\n",
    "            save_name=f\"{topic.replace(' ', '_')}_queries\",\n",
    "            temperature=CONSISTENT_TEMPERATURE,\n",
    "            model=model,\n",
    "        ) for topic, agent in topic_to_agent.items()\n",
    "    ])"
   ],
   "id": "2ac338d6d173bf0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Agent fine-tuning queries\n",
    "query_list_pattern = re.compile(r'\\[\\s*(\".*?\"\\s*(,\\s*\".*?\"\\s*)*)?,?\\s*\\]')\n",
    "\n",
    "topic_to_queries = {}\n",
    "\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    # Get query path for topic\n",
    "    query_path = finetuning_dir / f\"{topic.replace(' ', '_')}_queries.json\"\n",
    "\n",
    "    # Load query discussion\n",
    "    with open(query_path) as f:\n",
    "        query_discussion = json.load(f)\n",
    "\n",
    "    # Extract queries\n",
    "    query_message = query_discussion[-1][\"message\"]\n",
    "    pattern_result = query_list_pattern.search(query_message)\n",
    "\n",
    "    # Check if pattern is matched\n",
    "    if pattern_result is None:\n",
    "        print(f\"No queries found for {query_path}\")\n",
    "        continue\n",
    "\n",
    "    # Extract queries\n",
    "    queries = json.loads(pattern_result.group())\n",
    "    topic_to_queries[topic] = queries"
   ],
   "id": "603675b18435369e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Agent fine-tuning papers\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        concurrent.futures.wait([\n",
    "            executor.submit(\n",
    "                run_meeting,\n",
    "                meeting_type=\"individual\",\n",
    "                team_member=agent,\n",
    "                agenda=f\"{background_prompt} {nanobody_prompt} You are responsible for understanding the topic \\\"{topic}\\\" in the context of designing nanobody binders for SARS-CoV-2. You need to fine-tune yourself on the relevant literature on {topic} to improve your ability to design SARS-CoV-2 nanobody binders. Please use PubMed Central and search for relevant papers on {topic} using the query \\\"{query}\\\" and request 100 articles with abstracts only. Read all of the abstracts and based on each abstract individually, decide whether you want to fine-tune yourself on the full text of that paper. Include as many papers as possible, but only include papers that are directly relevant to {topic}. Please provide the PMCIDs and titles of all the papers that you wish to fine-tune yourself on as a Python dictionary mapping PMCID as a double-quoted string to title as a double-quoted string.\",\n",
    "                agenda_questions=(\n",
    "                    \"What are the PMCIDs and titles of the papers you wish to fine-tune yourself on (as a Python dictionary mapping PMCID as a double-quoted string to title as double-quoted string)?\",),\n",
    "                save_dir=finetuning_dir,\n",
    "                save_name=f\"{topic.replace(' ', '_')}_papers_{query_num + 1}\",\n",
    "                temperature=CONSISTENT_TEMPERATURE,\n",
    "                model=model,\n",
    "                pubmed_search=True,\n",
    "            ) for query_num, query in enumerate(topic_to_queries[topic]) if not (discussions_phase_to_dir[\n",
    "                                                                                     \"finetuning\"] / f\"{topic.replace(' ', '_')}_papers_{query_num + 1}.json\").exists()\n",
    "        ])"
   ],
   "id": "2ba3181c77e2e7b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract selected papers for fine-tuning\n",
    "pmcid_to_title_pattern = re.compile(r'\\{\\s*(\".*?\"\\s*:\\s*\".*?\"\\s*(,\\s*\".*?\"\\s*:\\s*\".*?\"\\s*)*)?\\}')\n",
    "\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    # Set up title to PMC ID dictionary\n",
    "    title_to_pmcid = {}\n",
    "    titles_lower, pmcids = set(), set()\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    # Get all paper paths for a topic\n",
    "    paper_paths = sorted(finetuning_dir.glob(f\"{topic_name}_papers_*.json\"))\n",
    "\n",
    "    # Check if all papers results are present\n",
    "    if len(paper_paths) != 5:\n",
    "        print(f\"Missing papers for {topic}\")\n",
    "        continue\n",
    "\n",
    "    # Extract PMC IDs and titles from each papers file\n",
    "    for paper_path in paper_paths:\n",
    "        # Load paper discussion\n",
    "        with open(paper_path) as f:\n",
    "            paper_discussion = json.load(f)\n",
    "\n",
    "        # Extract PMC IDs and titles dictionary\n",
    "        paper_message = paper_discussion[1][\"message\"]\n",
    "        pattern_result = pmcid_to_title_pattern.search(paper_message)\n",
    "\n",
    "        # Check if pattern is matched\n",
    "        if pattern_result is None:\n",
    "            print(f\"No papers found for {paper_path}\")\n",
    "            continue\n",
    "\n",
    "        # Extract PMC IDs and titles dictionary\n",
    "        pmcid_to_title = json.loads(pattern_result.group())\n",
    "\n",
    "        # Add PMC IDs and titles to dictionary, avoiding duplicates\n",
    "        for pmcid, title in pmcid_to_title.items():\n",
    "            # Replace en dash and em dash with a hyphen and convert to lowercase\n",
    "            title = title.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "            title_lower = title.lower()\n",
    "\n",
    "            if title_lower not in titles_lower and pmcid not in pmcids:\n",
    "                title_to_pmcid[title] = pmcid\n",
    "                titles_lower.add(title_lower)\n",
    "                pmcids.add(pmcid)\n",
    "\n",
    "    print(f\"Number of papers found for {topic}: {len(title_to_pmcid):,}\")\n",
    "\n",
    "    # Save title to PMC ID dictionary\n",
    "    with open(finetuning_dir / f\"{topic_name}_title_to_pmcid.json\", \"w\") as f:\n",
    "        json.dump(title_to_pmcid, f, indent=4, sort_keys=True)"
   ],
   "id": "b94895349ed6bb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get all PMCIDs\n",
    "pmcids = set()\n",
    "\n",
    "for topic in topic_to_agent:\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    with open(finetuning_dir / f\"{topic_name}_title_to_pmcid.json\") as f:\n",
    "        title_to_pmcid = json.load(f)\n",
    "\n",
    "    pmcids.update(title_to_pmcid.values())\n",
    "\n",
    "print(f\"Number of unique PMCIDs: {len(pmcids):,}\")"
   ],
   "id": "722a42e4e434064d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download papers from PubMed Central\n",
    "paper_count = 0\n",
    "\n",
    "for pmcid in tqdm(sorted(pmcids)):\n",
    "    title, content = get_pubmed_central_article(pmcid=pmcid)\n",
    "\n",
    "    if title is None:\n",
    "        continue\n",
    "\n",
    "    paper_count += 1\n",
    "\n",
    "    # Save paper\n",
    "    with open(papers_dir / f\"{pmcid}.json\", \"w\") as f:\n",
    "        json.dump({\"title\": title, \"content\": content}, f, indent=4, sort_keys=True)\n",
    "\n",
    "print(f\"Number of papers downloaded: {paper_count:,}\")"
   ],
   "id": "75555d31fe196365",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:21:09.587078Z",
     "start_time": "2025-01-17T20:21:09.583415Z"
    }
   },
   "cell_type": "code",
   "source": "%autoawait asyncio",
   "id": "67071da642a1501c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:21:13.237759Z",
     "start_time": "2025-01-17T20:21:13.232044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "async def summarize_paper(semaphore: asyncio.Semaphore, agent: Agent, topic: str, pmcid: str, title: str,\n",
    "                          content: list[str]) -> tuple[str, str, str]:\n",
    "    \"\"\"Summarize a paper using the model.\n",
    "\n",
    "    :param semaphore: Semaphore to limit the number of concurrent requests.\n",
    "    :param agent: Agent to use for summarization.\n",
    "    :param topic: Topic of interest.\n",
    "    :param pmcid: PMC ID of the paper.\n",
    "    :param title: Title of the paper.\n",
    "    :param content: Content of the paper.\n",
    "    :return: Tuple of PMC ID, title, and summary of the paper.\n",
    "    \"\"\"\n",
    "    # Set up query with paper\n",
    "    query = \"\\n\\n\".join([\n",
    "                            f\"Please summarize in extreme detail the following paper titled \\\"{title}\\\". Please focus in particular on summarizing key insights about the topic \\\"{topic}\\\" in relation to designing SARS-CoV-2 nanobody binders.\"] + content)\n",
    "\n",
    "    # Run query to get summary\n",
    "    async with semaphore:\n",
    "        assistant = await async_client.beta.assistants.create(name=agent.title, instructions=agent.prompt, model=model)\n",
    "        thread = await async_client.beta.threads.create()\n",
    "        await async_client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=query)\n",
    "        await async_client.beta.threads.runs.create_and_poll(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id,\n",
    "            model=model,\n",
    "            temperature=CONSISTENT_TEMPERATURE,\n",
    "        )\n",
    "        messages = await async_get_messages(client=async_client, thread_id=thread.id)\n",
    "        summary = messages[-1][\"content\"][0][\"text\"][\"value\"]\n",
    "\n",
    "    return pmcid, title, summary"
   ],
   "id": "5b0c48ff62aaf328",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:27:59.215709Z",
     "start_time": "2025-01-17T20:27:36.963752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create agent summaries of papers for fine-tuning\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    # Create save directory\n",
    "    topic_summary_dir = summaries_dir / topic_name\n",
    "    topic_summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load title to PMC ID dictionary\n",
    "    with open(finetuning_dir / f\"{topic_name}_title_to_pmcid.json\") as f:\n",
    "        title_to_pmcid: dict[str, str] = json.load(f)\n",
    "\n",
    "    # Get unique PMC IDs\n",
    "    pmcids = sorted(set((title_to_pmcid.values())))\n",
    "\n",
    "    # Load papers\n",
    "    pmcid_to_paper = {}\n",
    "    for pmcid in pmcids:\n",
    "        paper_path = papers_dir / f\"{pmcid}.json\"\n",
    "\n",
    "        if paper_path.exists():\n",
    "            with open(paper_path) as f:\n",
    "                paper: dict[str, str | list[str]] = json.load(f)\n",
    "                pmcid_to_paper[pmcid] = paper\n",
    "\n",
    "    print(f\"Number of papers loaded for {topic}: {len(pmcid_to_paper):,}\")\n",
    "\n",
    "    # Set up semaphore with the number of concurrent requests\n",
    "    semaphore = asyncio.Semaphore(num_concurrent)\n",
    "\n",
    "    # Create tasks for each paper\n",
    "    tasks = [\n",
    "        asyncio.create_task(\n",
    "            summarize_paper(semaphore=semaphore, agent=agent, topic=topic, pmcid=pmcid, title=paper[\"title\"],\n",
    "                            content=paper[\"content\"]))\n",
    "        for pmcid, paper in pmcid_to_paper.items()]\n",
    "\n",
    "    # Run agent summary of each paper\n",
    "    results = [(await task) for task in tqdm(asyncio.as_completed(tasks), total=len(tasks))]\n",
    "\n",
    "    # Save summaries\n",
    "    for pmcid, title, summary in results:\n",
    "        with open(topic_summary_dir / f\"{pmcid}.json\", \"w\") as f:\n",
    "            json.dump({\"pmcid\": pmcid, \"title\": title, \"summary\": summary}, f, indent=4, sort_keys=True)"
   ],
   "id": "58ad321787cb7e8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers loaded for nanobodies: 261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 32/261 [00:21<02:33,  1.49it/s] \n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Invalid 'content': string too long. Expected a string with maximum length 256000, but got a string with length 1671206 instead.\", 'type': 'invalid_request_error', 'param': 'content', 'code': 'string_above_max_length'}}",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mBadRequestError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 39\u001B[0m\n\u001B[1;32m     32\u001B[0m tasks \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     33\u001B[0m     asyncio\u001B[38;5;241m.\u001B[39mcreate_task(\n\u001B[1;32m     34\u001B[0m         summarize_paper(semaphore\u001B[38;5;241m=\u001B[39msemaphore, agent\u001B[38;5;241m=\u001B[39magent, topic\u001B[38;5;241m=\u001B[39mtopic, pmcid\u001B[38;5;241m=\u001B[39mpmcid, title\u001B[38;5;241m=\u001B[39mpaper[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtitle\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m     35\u001B[0m                         content\u001B[38;5;241m=\u001B[39mpaper[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m]))\n\u001B[1;32m     36\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m pmcid, paper \u001B[38;5;129;01min\u001B[39;00m pmcid_to_paper\u001B[38;5;241m.\u001B[39mitems()]\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m# Run agent summary of each paper\u001B[39;00m\n\u001B[0;32m---> 39\u001B[0m results \u001B[38;5;241m=\u001B[39m [(\u001B[38;5;28;01mawait\u001B[39;00m task) \u001B[38;5;28;01mfor\u001B[39;00m task \u001B[38;5;129;01min\u001B[39;00m tqdm(asyncio\u001B[38;5;241m.\u001B[39mas_completed(tasks), total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(tasks))]\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Save summaries\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m pmcid, title, summary \u001B[38;5;129;01min\u001B[39;00m results:\n",
      "File \u001B[0;32m~/anaconda3/envs/virtual_lab/lib/python3.12/asyncio/tasks.py:631\u001B[0m, in \u001B[0;36mas_completed.<locals>._wait_for_one\u001B[0;34m()\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m f \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# Dummy value from _on_timeout().\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTimeoutError\n\u001B[0;32m--> 631\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m f\u001B[38;5;241m.\u001B[39mresult()\n",
      "Cell \u001B[0;32mIn[5], line 19\u001B[0m, in \u001B[0;36msummarize_paper\u001B[0;34m(semaphore, agent, topic, pmcid, title, content)\u001B[0m\n\u001B[1;32m     17\u001B[0m assistant \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m async_client\u001B[38;5;241m.\u001B[39mbeta\u001B[38;5;241m.\u001B[39massistants\u001B[38;5;241m.\u001B[39mcreate(name\u001B[38;5;241m=\u001B[39magent\u001B[38;5;241m.\u001B[39mtitle, instructions\u001B[38;5;241m=\u001B[39magent\u001B[38;5;241m.\u001B[39mprompt, model\u001B[38;5;241m=\u001B[39mmodel)\n\u001B[1;32m     18\u001B[0m thread \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m async_client\u001B[38;5;241m.\u001B[39mbeta\u001B[38;5;241m.\u001B[39mthreads\u001B[38;5;241m.\u001B[39mcreate()\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m async_client\u001B[38;5;241m.\u001B[39mbeta\u001B[38;5;241m.\u001B[39mthreads\u001B[38;5;241m.\u001B[39mmessages\u001B[38;5;241m.\u001B[39mcreate(thread_id\u001B[38;5;241m=\u001B[39mthread\u001B[38;5;241m.\u001B[39mid, role\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m, content\u001B[38;5;241m=\u001B[39mquery)\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m async_client\u001B[38;5;241m.\u001B[39mbeta\u001B[38;5;241m.\u001B[39mthreads\u001B[38;5;241m.\u001B[39mruns\u001B[38;5;241m.\u001B[39mcreate_and_poll(\n\u001B[1;32m     21\u001B[0m     thread_id\u001B[38;5;241m=\u001B[39mthread\u001B[38;5;241m.\u001B[39mid,\n\u001B[1;32m     22\u001B[0m     assistant_id\u001B[38;5;241m=\u001B[39massistant\u001B[38;5;241m.\u001B[39mid,\n\u001B[1;32m     23\u001B[0m     model\u001B[38;5;241m=\u001B[39mmodel,\n\u001B[1;32m     24\u001B[0m     temperature\u001B[38;5;241m=\u001B[39mCONSISTENT_TEMPERATURE,\n\u001B[1;32m     25\u001B[0m )\n\u001B[1;32m     26\u001B[0m messages \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m async_get_messages(client\u001B[38;5;241m=\u001B[39masync_client, thread_id\u001B[38;5;241m=\u001B[39mthread\u001B[38;5;241m.\u001B[39mid)\n",
      "File \u001B[0;32m~/anaconda3/envs/virtual_lab/lib/python3.12/site-packages/openai/resources/beta/threads/messages.py:351\u001B[0m, in \u001B[0;36mAsyncMessages.create\u001B[0;34m(self, thread_id, content, role, attachments, metadata, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[1;32m    349\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected a non-empty value for `thread_id` but received \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mthread_id\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    350\u001B[0m extra_headers \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOpenAI-Beta\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124massistants=v2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m(extra_headers \u001B[38;5;129;01mor\u001B[39;00m {})}\n\u001B[0;32m--> 351\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_post(\n\u001B[1;32m    352\u001B[0m     \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/threads/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mthread_id\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/messages\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    353\u001B[0m     body\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mawait\u001B[39;00m async_maybe_transform(\n\u001B[1;32m    354\u001B[0m         {\n\u001B[1;32m    355\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcontent\u001B[39m\u001B[38;5;124m\"\u001B[39m: content,\n\u001B[1;32m    356\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrole\u001B[39m\u001B[38;5;124m\"\u001B[39m: role,\n\u001B[1;32m    357\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mattachments\u001B[39m\u001B[38;5;124m\"\u001B[39m: attachments,\n\u001B[1;32m    358\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    359\u001B[0m         },\n\u001B[1;32m    360\u001B[0m         message_create_params\u001B[38;5;241m.\u001B[39mMessageCreateParams,\n\u001B[1;32m    361\u001B[0m     ),\n\u001B[1;32m    362\u001B[0m     options\u001B[38;5;241m=\u001B[39mmake_request_options(\n\u001B[1;32m    363\u001B[0m         extra_headers\u001B[38;5;241m=\u001B[39mextra_headers, extra_query\u001B[38;5;241m=\u001B[39mextra_query, extra_body\u001B[38;5;241m=\u001B[39mextra_body, timeout\u001B[38;5;241m=\u001B[39mtimeout\n\u001B[1;32m    364\u001B[0m     ),\n\u001B[1;32m    365\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mMessage,\n\u001B[1;32m    366\u001B[0m )\n",
      "File \u001B[0;32m~/anaconda3/envs/virtual_lab/lib/python3.12/site-packages/openai/_base_client.py:1816\u001B[0m, in \u001B[0;36mAsyncAPIClient.post\u001B[0;34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001B[0m\n\u001B[1;32m   1802\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[1;32m   1803\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1804\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1811\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_AsyncStreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1812\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _AsyncStreamT:\n\u001B[1;32m   1813\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[1;32m   1814\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mawait\u001B[39;00m async_to_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[1;32m   1815\u001B[0m     )\n\u001B[0;32m-> 1816\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrequest(cast_to, opts, stream\u001B[38;5;241m=\u001B[39mstream, stream_cls\u001B[38;5;241m=\u001B[39mstream_cls)\n",
      "File \u001B[0;32m~/anaconda3/envs/virtual_lab/lib/python3.12/site-packages/openai/_base_client.py:1510\u001B[0m, in \u001B[0;36mAsyncAPIClient.request\u001B[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[1;32m   1502\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   1503\u001B[0m     cast_to: Type[ResponseT],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1508\u001B[0m     remaining_retries: Optional[\u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1509\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _AsyncStreamT:\n\u001B[0;32m-> 1510\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_request(\n\u001B[1;32m   1511\u001B[0m         cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m   1512\u001B[0m         options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   1513\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream,\n\u001B[1;32m   1514\u001B[0m         stream_cls\u001B[38;5;241m=\u001B[39mstream_cls,\n\u001B[1;32m   1515\u001B[0m         remaining_retries\u001B[38;5;241m=\u001B[39mremaining_retries,\n\u001B[1;32m   1516\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/envs/virtual_lab/lib/python3.12/site-packages/openai/_base_client.py:1611\u001B[0m, in \u001B[0;36mAsyncAPIClient._request\u001B[0;34m(self, cast_to, options, stream, stream_cls, remaining_retries)\u001B[0m\n\u001B[1;32m   1608\u001B[0m         \u001B[38;5;28;01mawait\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39maread()\n\u001B[1;32m   1610\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1611\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1613\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[1;32m   1614\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[1;32m   1615\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1619\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39moptions\u001B[38;5;241m.\u001B[39mget_max_retries(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries) \u001B[38;5;241m-\u001B[39m retries,\n\u001B[1;32m   1620\u001B[0m )\n",
      "\u001B[0;31mBadRequestError\u001B[0m: Error code: 400 - {'error': {'message': \"Invalid 'content': string too long. Expected a string with maximum length 256000, but got a string with length 1671206 instead.\", 'type': 'invalid_request_error', 'param': 'content', 'code': 'string_above_max_length'}}"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-17T20:30:44.533645Z",
     "start_time": "2025-01-17T20:30:43.506350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert summaries to training data format\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    # Get summary paths\n",
    "    topic_summary_dir = summaries_dir / topic_name\n",
    "    summary_paths = sorted(topic_summary_dir.glob(\"*.json\"))\n",
    "\n",
    "    # Convert summaries to training data format\n",
    "    training_data = []\n",
    "\n",
    "    for summary_path in summary_paths:\n",
    "        # Load paper summary data\n",
    "        with open(summary_path) as f:\n",
    "            summary_data = json.load(f)\n",
    "\n",
    "        # Extract title and summary\n",
    "        title, summary = summary_data[\"title\"], summary_data[\"summary\"]\n",
    "\n",
    "        # Add example to training data\n",
    "        training_data.append(\n",
    "            {\"messages\": [{\"role\": \"system\", \"content\": agent.prompt},\n",
    "                          {\"role\": \"user\",\n",
    "                           \"content\": f\"Please tell me about the paper \\\"{title}\\\" and its insights into \\\"{topic}\\\" in relation to designing SARS-CoV-2 nanobody binders.\"},\n",
    "                          {\"role\": \"assistant\", \"content\": summary}]}\n",
    "        )\n",
    "\n",
    "    # Count tokens\n",
    "    token_count = [sum(count_tokens(message[\"content\"]) for message in data[\"messages\"]) for data in training_data]\n",
    "\n",
    "    # TODO: estimate fine-tuning pricing\n",
    "\n",
    "    # Print stats\n",
    "    print(f\"Number of paper examples for {topic}: {len(training_data):,}\")\n",
    "    print(f\"Token count for {topic}: {sum(token_count):,}\")\n",
    "\n",
    "    # Save training data in jsonl format\n",
    "    with open(finetuning_dir / f\"{topic_name}_training_data.jsonl\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(json.dumps(example) for example in training_data))"
   ],
   "id": "9ab5352bf603e433",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of paper examples for ESM: 34\n",
      "Token count for ESM: 27,518\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Upload fine-tuning data\n",
    "topic_to_id = {}\n",
    "\n",
    "for topic in topic_to_agent:\n",
    "    path = finetuning_dir / f\"{topic.replace(' ', '_')}_training_data.jsonl\"\n",
    "\n",
    "    file_object = client.files.create(\n",
    "        file=open(path, \"rb\"),\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "\n",
    "    topic_to_id[topic] = file_object.id\n",
    "\n",
    "# Save topic file IDs\n",
    "with open(finetuning_dir / \"topic_to_id.json\", \"w\") as f:\n",
    "    json.dump(topic_to_id, f)"
   ],
   "id": "ee5c062a1cd23206",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load topic file IDs\n",
    "with open(finetuning_dir / \"topic_to_id.json\") as f:\n",
    "    topic_to_id = json.load(f)\n",
    "\n",
    "topics = sorted(topic_to_id)"
   ],
   "id": "bb0e6da655cb759a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Launch fine-tuning jobs\n",
    "# for topic, file_id in topic_to_id.items():\n",
    "#     client.fine_tuning.jobs.create(\n",
    "#         training_file=file_id,\n",
    "#         model=finetuning_model,\n",
    "#     )\n",
    "\n",
    "topic = topics[5]\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "    training_file=topic_to_id[topic],\n",
    "    model=finetuning_base_model,  # TODO: swap for GPT-4o, not mini\n",
    "    suffix=topic.replace(\" \", \"_\"),\n",
    ")"
   ],
   "id": "7da9369b7c17cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(list(client.fine_tuning.jobs.list())[1].fine_tuned_model)",
   "id": "66ae2d0e00cb8e3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set up topic to fine-tuned model mapping\n",
    "topic_to_model = {\n",
    "    \"AlphaFold-Multimer\": \"ft:gpt-4o-mini-2024-07-18:personal:alphafold-multimer:AmtqgHON\",\n",
    "    \"ESM\": \"ft:gpt-4o-mini-2024-07-18:personal:esm:AmtjuDox\",\n",
    "    \"Rosetta\": \"ft:gpt-4o-mini-2024-07-18:personal:rosetta:Amtuos8C\",\n",
    "    \"SARS-CoV-2 spike protein\": \"ft:gpt-4o-mini-2024-07-18:personal:sars-cov-2-spike-protein:AmuRh1c1\",\n",
    "    \"SARS-CoV-2 variants KP.3 and JN.1\": \"ft:gpt-4o-mini-2024-07-18:personal:sars-cov-2-variants-kp-3-and-jn-1:AmyELRn8\",\n",
    "    \"nanobodies\": \"ft:gpt-4o-mini-2024-07-18:personal:nanobodies:AmyVyYww\",\n",
    "}"
   ],
   "id": "295b1b032e55cc98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "topic = \"SARS-CoV-2 variants KP.3 and JN.1\"\n",
    "agent = topic_to_agent[topic]\n",
    "query = \"How are the JN.1 and KP.3 variants of SARS-CoV-2 related to each other?\"\n",
    "\n",
    "for selected_model, selected_model_name in [(finetuning_base_model, \"base model\"),\n",
    "                                            (topic_to_model[topic], \"fine-tuned model\")]:\n",
    "    print(f\"Running query \\\"{query}\\\" with {selected_model_name} for {topic}.\\n\")\n",
    "\n",
    "    assistant = client.beta.assistants.create(name=agent.title, instructions=agent.prompt, model=selected_model)\n",
    "    thread = client.beta.threads.create()\n",
    "    client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=query)\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        model=selected_model,\n",
    "        temperature=CONSISTENT_TEMPERATURE,\n",
    "    )\n",
    "\n",
    "    print(run.status)\n",
    "\n",
    "    # if not run.status == \"complete\":\n",
    "    #     print(\"Query failed to complete.\")\n",
    "    #     continue\n",
    "\n",
    "    messages = get_messages(client=client, thread_id=thread.id)\n",
    "\n",
    "    print(messages[-1][\"content\"][0][\"text\"][\"value\"])\n",
    "    print()"
   ],
   "id": "51a0bdefdd05dd63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "77b7c3f7cde6f733",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
