{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T01:38:07.956071Z",
     "start_time": "2025-01-07T01:38:06.748322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import re\n",
    "\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "from virtual_lab.constants import CONSISTENT_TEMPERATURE\n",
    "from virtual_lab.run_meeting import run_meeting\n",
    "from virtual_lab.utils import get_messages, get_pubmed_central_article\n",
    "\n",
    "from nanobody_constants import (\n",
    "    background_prompt,\n",
    "    nanobody_prompt,\n",
    "    discussions_phase_to_dir,\n",
    "    model,\n",
    "    finetuning_base_model,\n",
    "    immunologist,\n",
    "    machine_learning_specialist,\n",
    "    computational_biologist,\n",
    ")"
   ],
   "id": "a0472e92df8ee037",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T01:38:07.963937Z",
     "start_time": "2025-01-07T01:38:07.961178Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Topic to agent mapping\n",
    "topic_to_agent = {\n",
    "    \"nanobodies\": immunologist,\n",
    "    \"SARS-CoV-2 spike protein\": immunologist,\n",
    "    \"SARS-CoV-2 variants KP.3 and JN.1\": immunologist,\n",
    "    \"ESM\": machine_learning_specialist,\n",
    "    \"AlphaFold-Multimer\": computational_biologist,\n",
    "    \"Rosetta\": computational_biologist,\n",
    "}"
   ],
   "id": "164ca17b6c09fc08",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Agent fine-tuning queries\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    concurrent.futures.wait([\n",
    "        executor.submit(\n",
    "            run_meeting,\n",
    "            meeting_type=\"individual\",\n",
    "            team_member=agent,\n",
    "            agenda=f\"{background_prompt} {nanobody_prompt} You are responsible for understanding the topic \\\"{topic}\\\" in the context of designing nanobody binders for SARS-CoV-2. You need to fine-tune yourself on the relevant literature on {topic} to improve your ability to design SARS-CoV-2 nanobody binders. Please write out a series of five distinct search queries that you want to run to find relevant scientific papers on {topic}. Include both queries about {topic} generally as well as queries about how {topic} relates to designing nanobody binders for SARS-CoV-2. Please provide the queries in Python syntax as a list of double-quoted strings.\",\n",
    "            agenda_questions=(\n",
    "                f\"What are the queries that you want to perform to identify the relevant literature on {topic} (as a list of double-quoted strings in Python syntax)?\",),\n",
    "            save_dir=discussions_phase_to_dir[\"finetuning\"],\n",
    "            save_name=f\"{topic.replace(' ', '_')}_queries\",\n",
    "            temperature=CONSISTENT_TEMPERATURE,\n",
    "            model=model,\n",
    "        ) for topic, agent in topic_to_agent.items()\n",
    "    ])"
   ],
   "id": "2ac338d6d173bf0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T01:38:09.726362Z",
     "start_time": "2025-01-07T01:38:09.717074Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Agent fine-tuning queries\n",
    "query_list_pattern = re.compile(r'\\[\\s*(\".*?\"\\s*(,\\s*\".*?\"\\s*)*)?,?\\s*\\]')\n",
    "\n",
    "topic_to_queries = {}\n",
    "\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    # Get query path for topic\n",
    "    query_path = discussions_phase_to_dir[\"finetuning\"] / f\"{topic.replace(' ', '_')}_queries.json\"\n",
    "\n",
    "    # Load query discussion\n",
    "    with open(query_path) as f:\n",
    "        query_discussion = json.load(f)\n",
    "\n",
    "    # Extract queries\n",
    "    query_message = query_discussion[-1][\"message\"]\n",
    "    pattern_result = query_list_pattern.search(query_message)\n",
    "\n",
    "    # Check if pattern is matched\n",
    "    if pattern_result is None:\n",
    "        print(f\"No queries found for {query_path}\")\n",
    "        continue\n",
    "\n",
    "    # Extract queries\n",
    "    queries = json.loads(pattern_result.group())\n",
    "    topic_to_queries[topic] = queries"
   ],
   "id": "603675b18435369e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Agent fine-tuning papers\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        concurrent.futures.wait([\n",
    "            executor.submit(\n",
    "                run_meeting,\n",
    "                meeting_type=\"individual\",\n",
    "                team_member=agent,\n",
    "                agenda=f\"{background_prompt} {nanobody_prompt} You are responsible for understanding the topic \\\"{topic}\\\" in the context of designing nanobody binders for SARS-CoV-2. You need to fine-tune yourself on the relevant literature on {topic} to improve your ability to design SARS-CoV-2 nanobody binders. Please use PubMed Central and search for relevant papers on {topic} using the query \\\"{query}\\\" and request 100 articles with abstracts only. Read all of the abstracts and based on each abstract individually, decide whether you want to fine-tune yourself on the full text of that paper. Include as many papers as possible, but only include papers that are directly relevant to {topic}. Please provide the PMCIDs and titles of all the papers that you wish to fine-tune yourself on as a Python dictionary mapping PMCID as a double-quoted string to title as a double-quoted string.\",\n",
    "                agenda_questions=(\n",
    "                    \"What are the PMCIDs and titles of the papers you wish to fine-tune yourself on (as a Python dictionary mapping PMCID as a double-quoted string to title as double-quoted string)?\",),\n",
    "                save_dir=discussions_phase_to_dir[\"finetuning\"],\n",
    "                save_name=f\"{topic.replace(' ', '_')}_papers_{query_num + 1}\",\n",
    "                temperature=CONSISTENT_TEMPERATURE,\n",
    "                model=model,\n",
    "                pubmed_search=True,\n",
    "            ) for query_num, query in enumerate(topic_to_queries[topic]) if not (discussions_phase_to_dir[\n",
    "                                                                                     \"finetuning\"] / f\"{topic.replace(' ', '_')}_papers_{query_num + 1}.json\").exists()\n",
    "        ])"
   ],
   "id": "2ba3181c77e2e7b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T01:54:27.914405Z",
     "start_time": "2025-01-07T01:54:27.855913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract selected papers for fine-tuning\n",
    "pmcid_to_title_pattern = re.compile(r'\\{\\s*(\".*?\"\\s*:\\s*\".*?\"\\s*(,\\s*\".*?\"\\s*:\\s*\".*?\"\\s*)*)?\\}')\n",
    "\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    # Set up title to PMC ID dictionary\n",
    "    title_to_pmcid = {}\n",
    "    titles_lower, pmcids = set(), set()\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    # Get all paper paths for a topic\n",
    "    paper_paths = sorted(discussions_phase_to_dir[\"finetuning\"].glob(f\"{topic_name}_papers_*.json\"))\n",
    "\n",
    "    # Check if all papers results are present\n",
    "    if len(paper_paths) != 5:\n",
    "        print(f\"Missing papers for {topic}\")\n",
    "        continue\n",
    "\n",
    "    # Extract PMC IDs and titles from each papers file\n",
    "    for paper_path in paper_paths:\n",
    "        # Load paper discussion\n",
    "        with open(paper_path) as f:\n",
    "            paper_discussion = json.load(f)\n",
    "\n",
    "        # Extract PMC IDs and titles dictionary\n",
    "        paper_message = paper_discussion[1][\"message\"]\n",
    "        pattern_result = pmcid_to_title_pattern.search(paper_message)\n",
    "\n",
    "        # Check if pattern is matched\n",
    "        if pattern_result is None:\n",
    "            print(f\"No papers found for {paper_path}\")\n",
    "            continue\n",
    "\n",
    "        # Extract PMC IDs and titles dictionary\n",
    "        pmcid_to_title = json.loads(pattern_result.group())\n",
    "\n",
    "        # Add PMC IDs and titles to dictionary, avoiding duplicates\n",
    "        for pmcid, title in pmcid_to_title.items():\n",
    "            # Replace en dash and em dash with a hyphen and convert to lowercase\n",
    "            title = title.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "            title_lower = title.lower()\n",
    "\n",
    "            if title_lower not in titles_lower and pmcid not in pmcids:\n",
    "                title_to_pmcid[title] = pmcid\n",
    "                titles_lower.add(title_lower)\n",
    "                pmcids.add(pmcid)\n",
    "\n",
    "    print(f\"Number of papers for {topic}: {len(title_to_pmcid):,}\")\n",
    "\n",
    "    # Save title to PMC ID dictionary\n",
    "    with open(discussions_phase_to_dir[\"finetuning\"] / f\"{topic_name}_title_to_pmcid.json\", \"w\") as f:\n",
    "        json.dump(title_to_pmcid, f, indent=4, sort_keys=True)"
   ],
   "id": "b94895349ed6bb8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers for nanobodies: 261\n",
      "Number of papers for SARS-CoV-2 spike protein: 331\n",
      "No papers found for discussions/finetuning/SARS-CoV-2_variants_KP.3_and_JN.1_papers_5.json\n",
      "Number of papers for SARS-CoV-2 variants KP.3 and JN.1: 24\n",
      "Number of papers for ESM: 34\n",
      "Number of papers for AlphaFold-Multimer: 112\n",
      "Number of papers for Rosetta: 113\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T02:04:05.568880Z",
     "start_time": "2025-01-07T01:57:11.190309Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download papers from PubMed Central for fine-tuning\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    # Load title to PMC ID dictionary\n",
    "    with open(discussions_phase_to_dir[\"finetuning\"] / f\"{topic_name}_title_to_pmcid.json\") as f:\n",
    "        title_to_pmcid = json.load(f)\n",
    "\n",
    "    # Get unique PMC IDs\n",
    "    pmcids = sorted(set((title_to_pmcid.values())))\n",
    "\n",
    "    # Download papers by PMC ID and format for fine-tuning\n",
    "    training_data = []\n",
    "    for pmcid in tqdm(pmcids):\n",
    "        title, content = get_pubmed_central_article(pmcid=pmcid)\n",
    "\n",
    "        if title is None:\n",
    "            continue\n",
    "\n",
    "        training_data += [\n",
    "            {\"messages\": [{\"role\": \"system\", \"content\": agent.prompt},\n",
    "                          {\"role\": \"user\", \"content\": \"\"},\n",
    "                          {\"role\": \"assistant\", \"content\": paragraph}]}\n",
    "            for paragraph in [title] + content\n",
    "        ]\n",
    "\n",
    "    print(f\"Number of full text papers for {topic}: {len(pmcids):,}\")\n",
    "    print(f\"Number of paragraph examples for {topic}: {len(training_data):,}\")\n",
    "\n",
    "    # Save training data in jsonl format\n",
    "    with open(discussions_phase_to_dir[\"finetuning\"] / f\"{topic.replace(' ', '_')}_training_data.jsonl\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(json.dumps(example) for example in training_data))"
   ],
   "id": "75555d31fe196365",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 261/261 [02:07<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of full text papers for nanobodies: 261\n",
      "Number of paragraph examples for nanobodies: 36,715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 331/331 [02:38<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of full text papers for SARS-CoV-2 spike protein: 331\n",
      "Number of paragraph examples for SARS-CoV-2 spike protein: 46,472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:10<00:00,  2.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of full text papers for SARS-CoV-2 variants KP.3 and JN.1: 24\n",
      "Number of paragraph examples for SARS-CoV-2 variants KP.3 and JN.1: 926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 34/34 [00:15<00:00,  2.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of full text papers for ESM: 34\n",
      "Number of paragraph examples for ESM: 1,853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112/112 [00:49<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of full text papers for AlphaFold-Multimer: 112\n",
      "Number of paragraph examples for AlphaFold-Multimer: 6,478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 113/113 [00:52<00:00,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of full text papers for Rosetta: 113\n",
      "Number of paragraph examples for Rosetta: 5,890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T02:04:16.926597Z",
     "start_time": "2025-01-07T02:04:16.877949Z"
    }
   },
   "cell_type": "code",
   "source": "client = OpenAI()",
   "id": "2b13849c6c4f5802",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T02:04:34.452704Z",
     "start_time": "2025-01-07T02:04:20.259225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Upload fine-tuning data\n",
    "topic_to_id = {}\n",
    "\n",
    "for topic in topic_to_agent:\n",
    "    path = discussions_phase_to_dir[\"finetuning\"] / f\"{topic.replace(' ', '_')}_training_data.jsonl\"\n",
    "\n",
    "    file_object = client.files.create(\n",
    "        file=open(path, \"rb\"),\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "\n",
    "    topic_to_id[topic] = file_object.id\n",
    "\n",
    "# Save topic file IDs\n",
    "with open(discussions_phase_to_dir[\"finetuning\"] / \"topic_to_id.json\", \"w\") as f:\n",
    "    json.dump(topic_to_id, f)"
   ],
   "id": "ee5c062a1cd23206",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T02:04:39.457491Z",
     "start_time": "2025-01-07T02:04:39.454263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load topic file IDs\n",
    "with open(discussions_phase_to_dir[\"finetuning\"] / \"topic_to_id.json\") as f:\n",
    "    topic_to_id = json.load(f)\n",
    "\n",
    "topics = sorted(topic_to_id)"
   ],
   "id": "bb0e6da655cb759a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T02:05:04.600320Z",
     "start_time": "2025-01-07T02:05:04.001641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Launch fine-tuning jobs\n",
    "# for topic, file_id in topic_to_id.items():\n",
    "#     client.fine_tuning.jobs.create(\n",
    "#         training_file=file_id,\n",
    "#         model=finetuning_model,\n",
    "#     )\n",
    "\n",
    "topic = topics[3]\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "    training_file=topic_to_id[topic],\n",
    "    model=finetuning_base_model,  # TODO: swap for GPT-4o, not mini\n",
    "    suffix=topic.replace(\" \", \"_\"),\n",
    ")"
   ],
   "id": "7da9369b7c17cd2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FineTuningJob(id='ftjob-cM8IxrOQCAnKnfZwsZG67NVn', created_at=1736215504, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs='auto', batch_size='auto', learning_rate_multiplier='auto'), model='gpt-4o-mini-2024-07-18', object='fine_tuning.job', organization_id='org-JdFlu7kGMrLJr0BRViNsU6SO', result_files=[], seed=2049714135, status='validating_files', trained_tokens=None, training_file='file-HxGMGMpgR4RtrSJrAv3dPL', validation_file=None, estimated_finish=None, integrations=[], user_provided_suffix='SARS-CoV-2_spike_protein', method={'type': 'supervised', 'supervised': {'hyperparameters': {'batch_size': 'auto', 'learning_rate_multiplier': 'auto', 'n_epochs': 'auto'}}})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-07T02:05:05.055477Z",
     "start_time": "2025-01-07T02:05:04.683705Z"
    }
   },
   "cell_type": "code",
   "source": "list(client.fine_tuning.jobs.list())[0].status",
   "id": "66ae2d0e00cb8e3e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'validating_files'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-04T21:14:58.154162Z",
     "start_time": "2025-01-04T21:14:58.150481Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set up topic to fine-tuned model mapping\n",
    "# TODO: update model IDs\n",
    "topic_to_model = {\n",
    "    \"AlphaFold-Multimer\": \"ft:gpt-4o-mini-2024-07-18:personal::AlsW3uTg\",\n",
    "    \"ESM\": \"ft:gpt-4o-mini-2024-07-18:personal::AlrtT5SS\",\n",
    "    \"Rosetta\": \"ft:gpt-4o-mini-2024-07-18:personal::AlsI0vLG\",\n",
    "    \"SARS-CoV-2 spike protein\": \"ft:gpt-4o-mini-2024-07-18:personal::Am2Y4mU1\",\n",
    "    \"SARS-CoV-2 variants KP.3 and JN.1\": \"ft:gpt-4o-mini-2024-07-18:personal::Am1FIJLR\",\n",
    "    \"nanobodies\": \"ft:gpt-4o-mini-2024-07-18:personal::Am1xzqlG\",\n",
    "}"
   ],
   "id": "295b1b032e55cc98",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "topic = \"ESM\"\n",
    "\n",
    "selected_model = finetuning_base_model\n",
    "# selected_model = topic_to_model[topic]\n",
    "agent = topic_to_agent[topic]\n",
    "query = \"What are the three algorithms of the DeepRank-GNN-esm model?\"\n",
    "\n",
    "assistant = client.beta.assistants.create(name=agent.title, instructions=agent.prompt, model=selected_model)\n",
    "thread = client.beta.threads.create()\n",
    "client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=query)\n",
    "run = client.beta.threads.runs.create_and_poll(\n",
    "    thread_id=thread.id,\n",
    "    assistant_id=assistant.id,\n",
    "    model=selected_model,\n",
    "    temperature=CONSISTENT_TEMPERATURE,\n",
    ")\n",
    "messages = get_messages(client=client, thread_id=thread.id)\n",
    "\n",
    "print(messages[-1][\"content\"][0][\"text\"][\"value\"])"
   ],
   "id": "51a0bdefdd05dd63",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
