{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import concurrent.futures\n",
    "import json\n",
    "import re\n",
    "\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "from virtual_lab.constants import CONSISTENT_TEMPERATURE\n",
    "from virtual_lab.run_meeting import run_meeting\n",
    "from virtual_lab.utils import get_messages, get_pubmed_central_article\n",
    "\n",
    "from nanobody_constants import (\n",
    "    background_prompt,\n",
    "    nanobody_prompt,\n",
    "    discussions_phase_to_dir,\n",
    "    model,\n",
    "    finetuning_base_model,\n",
    "    immunologist,\n",
    "    machine_learning_specialist,\n",
    "    computational_biologist,\n",
    ")"
   ],
   "id": "a0472e92df8ee037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "finetuning_dir = discussions_phase_to_dir[\"finetuning\"]\n",
    "papers_dir = finetuning_dir / \"papers\"\n",
    "summaries_dir = finetuning_dir / \"summaries\"\n",
    "\n",
    "papers_dir.mkdir(parents=True, exist_ok=True)\n",
    "summaries_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "client = OpenAI()"
   ],
   "id": "fc28ade3791ee419",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Topic to agent mapping\n",
    "topic_to_agent = {\n",
    "    \"nanobodies\": immunologist,\n",
    "    \"SARS-CoV-2 spike protein\": immunologist,\n",
    "    \"SARS-CoV-2 variants KP.3 and JN.1\": immunologist,\n",
    "    \"ESM\": machine_learning_specialist,\n",
    "    \"AlphaFold-Multimer\": computational_biologist,\n",
    "    \"Rosetta\": computational_biologist,\n",
    "}"
   ],
   "id": "164ca17b6c09fc08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Agent fine-tuning queries\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    concurrent.futures.wait([\n",
    "        executor.submit(\n",
    "            run_meeting,\n",
    "            meeting_type=\"individual\",\n",
    "            team_member=agent,\n",
    "            agenda=f\"{background_prompt} {nanobody_prompt} You are responsible for understanding the topic \\\"{topic}\\\" in the context of designing nanobody binders for SARS-CoV-2. You need to fine-tune yourself on the relevant literature on {topic} to improve your ability to design SARS-CoV-2 nanobody binders. Please write out a series of five distinct search queries that you want to run to find relevant scientific papers on {topic}. Include both queries about {topic} generally as well as queries about how {topic} relates to designing nanobody binders for SARS-CoV-2. Please provide the queries in Python syntax as a list of double-quoted strings.\",\n",
    "            agenda_questions=(\n",
    "                f\"What are the queries that you want to perform to identify the relevant literature on {topic} (as a list of double-quoted strings in Python syntax)?\",),\n",
    "            save_dir=finetuning_dir,\n",
    "            save_name=f\"{topic.replace(' ', '_')}_queries\",\n",
    "            temperature=CONSISTENT_TEMPERATURE,\n",
    "            model=model,\n",
    "        ) for topic, agent in topic_to_agent.items()\n",
    "    ])"
   ],
   "id": "2ac338d6d173bf0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Agent fine-tuning queries\n",
    "query_list_pattern = re.compile(r'\\[\\s*(\".*?\"\\s*(,\\s*\".*?\"\\s*)*)?,?\\s*\\]')\n",
    "\n",
    "topic_to_queries = {}\n",
    "\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    # Get query path for topic\n",
    "    query_path = finetuning_dir / f\"{topic.replace(' ', '_')}_queries.json\"\n",
    "\n",
    "    # Load query discussion\n",
    "    with open(query_path) as f:\n",
    "        query_discussion = json.load(f)\n",
    "\n",
    "    # Extract queries\n",
    "    query_message = query_discussion[-1][\"message\"]\n",
    "    pattern_result = query_list_pattern.search(query_message)\n",
    "\n",
    "    # Check if pattern is matched\n",
    "    if pattern_result is None:\n",
    "        print(f\"No queries found for {query_path}\")\n",
    "        continue\n",
    "\n",
    "    # Extract queries\n",
    "    queries = json.loads(pattern_result.group())\n",
    "    topic_to_queries[topic] = queries"
   ],
   "id": "603675b18435369e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Agent fine-tuning papers\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        concurrent.futures.wait([\n",
    "            executor.submit(\n",
    "                run_meeting,\n",
    "                meeting_type=\"individual\",\n",
    "                team_member=agent,\n",
    "                agenda=f\"{background_prompt} {nanobody_prompt} You are responsible for understanding the topic \\\"{topic}\\\" in the context of designing nanobody binders for SARS-CoV-2. You need to fine-tune yourself on the relevant literature on {topic} to improve your ability to design SARS-CoV-2 nanobody binders. Please use PubMed Central and search for relevant papers on {topic} using the query \\\"{query}\\\" and request 100 articles with abstracts only. Read all of the abstracts and based on each abstract individually, decide whether you want to fine-tune yourself on the full text of that paper. Include as many papers as possible, but only include papers that are directly relevant to {topic}. Please provide the PMCIDs and titles of all the papers that you wish to fine-tune yourself on as a Python dictionary mapping PMCID as a double-quoted string to title as a double-quoted string.\",\n",
    "                agenda_questions=(\n",
    "                    \"What are the PMCIDs and titles of the papers you wish to fine-tune yourself on (as a Python dictionary mapping PMCID as a double-quoted string to title as double-quoted string)?\",),\n",
    "                save_dir=finetuning_dir,\n",
    "                save_name=f\"{topic.replace(' ', '_')}_papers_{query_num + 1}\",\n",
    "                temperature=CONSISTENT_TEMPERATURE,\n",
    "                model=model,\n",
    "                pubmed_search=True,\n",
    "            ) for query_num, query in enumerate(topic_to_queries[topic]) if not (discussions_phase_to_dir[\n",
    "                                                                                     \"finetuning\"] / f\"{topic.replace(' ', '_')}_papers_{query_num + 1}.json\").exists()\n",
    "        ])"
   ],
   "id": "2ba3181c77e2e7b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Extract selected papers for fine-tuning\n",
    "pmcid_to_title_pattern = re.compile(r'\\{\\s*(\".*?\"\\s*:\\s*\".*?\"\\s*(,\\s*\".*?\"\\s*:\\s*\".*?\"\\s*)*)?\\}')\n",
    "\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    # Set up title to PMC ID dictionary\n",
    "    title_to_pmcid = {}\n",
    "    titles_lower, pmcids = set(), set()\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    # Get all paper paths for a topic\n",
    "    paper_paths = sorted(finetuning_dir.glob(f\"{topic_name}_papers_*.json\"))\n",
    "\n",
    "    # Check if all papers results are present\n",
    "    if len(paper_paths) != 5:\n",
    "        print(f\"Missing papers for {topic}\")\n",
    "        continue\n",
    "\n",
    "    # Extract PMC IDs and titles from each papers file\n",
    "    for paper_path in paper_paths:\n",
    "        # Load paper discussion\n",
    "        with open(paper_path) as f:\n",
    "            paper_discussion = json.load(f)\n",
    "\n",
    "        # Extract PMC IDs and titles dictionary\n",
    "        paper_message = paper_discussion[1][\"message\"]\n",
    "        pattern_result = pmcid_to_title_pattern.search(paper_message)\n",
    "\n",
    "        # Check if pattern is matched\n",
    "        if pattern_result is None:\n",
    "            print(f\"No papers found for {paper_path}\")\n",
    "            continue\n",
    "\n",
    "        # Extract PMC IDs and titles dictionary\n",
    "        pmcid_to_title = json.loads(pattern_result.group())\n",
    "\n",
    "        # Add PMC IDs and titles to dictionary, avoiding duplicates\n",
    "        for pmcid, title in pmcid_to_title.items():\n",
    "            # Replace en dash and em dash with a hyphen and convert to lowercase\n",
    "            title = title.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "            title_lower = title.lower()\n",
    "\n",
    "            if title_lower not in titles_lower and pmcid not in pmcids:\n",
    "                title_to_pmcid[title] = pmcid\n",
    "                titles_lower.add(title_lower)\n",
    "                pmcids.add(pmcid)\n",
    "\n",
    "    print(f\"Number of papers found for {topic}: {len(title_to_pmcid):,}\")\n",
    "\n",
    "    # Save title to PMC ID dictionary\n",
    "    with open(finetuning_dir / f\"{topic_name}_title_to_pmcid.json\", \"w\") as f:\n",
    "        json.dump(title_to_pmcid, f, indent=4, sort_keys=True)"
   ],
   "id": "b94895349ed6bb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get all PMCIDs\n",
    "pmcids = set()\n",
    "\n",
    "for topic in topic_to_agent:\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    with open(finetuning_dir / f\"{topic_name}_title_to_pmcid.json\") as f:\n",
    "        title_to_pmcid = json.load(f)\n",
    "\n",
    "    pmcids.update(title_to_pmcid.values())\n",
    "\n",
    "print(f\"Number of unique PMCIDs: {len(pmcids):,}\")"
   ],
   "id": "722a42e4e434064d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Download papers from PubMed Central\n",
    "paper_count = 0\n",
    "\n",
    "for pmcid in tqdm(sorted(pmcids)):\n",
    "    title, content = get_pubmed_central_article(pmcid=pmcid)\n",
    "\n",
    "    if title is None:\n",
    "        continue\n",
    "\n",
    "    paper_count += 1\n",
    "\n",
    "    # Save paper\n",
    "    with open(papers_dir / f\"{pmcid}.json\", \"w\") as f:\n",
    "        json.dump({\"title\": title, \"content\": content}, f, indent=4, sort_keys=True)\n",
    "\n",
    "print(f\"Number of papers downloaded: {paper_count:,}\")"
   ],
   "id": "75555d31fe196365",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create agent summaries of papers for fine-tuning\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    # Create save directory\n",
    "    topic_summary_dir = summaries_dir / topic_name\n",
    "\n",
    "    # Load title to PMC ID dictionary\n",
    "    with open(finetuning_dir / f\"{topic_name}_title_to_pmcid.json\") as f:\n",
    "        title_to_pmcid: dict[str, str] = json.load(f)\n",
    "\n",
    "    # Get unique PMC IDs\n",
    "    pmcids = sorted(set((title_to_pmcid.values())))\n",
    "\n",
    "    # Load papers\n",
    "    pmcid_to_paper = {}\n",
    "    for pmcid in pmcids:\n",
    "        paper_path = papers_dir / f\"{pmcid}.json\"\n",
    "\n",
    "        if paper_path.exists():\n",
    "            with open(paper_path) as f:\n",
    "                paper: dict[str, str | list[str]] = json.load(f)\n",
    "                pmcid_to_paper[pmcid] = paper\n",
    "\n",
    "    print(f\"Number of papers loaded for {topic}: {len(pmcid_to_paper):,}\")\n",
    "\n",
    "    # Create assistant\n",
    "    assistant = client.beta.assistants.create(name=agent.title, instructions=agent.prompt, model=model)\n",
    "\n",
    "    # TODO: parallelize\n",
    "\n",
    "    # Create agent summary of each paper\n",
    "    for pmcid, paper in pmcid_to_paper.items():\n",
    "        # Set up summary query\n",
    "        title, content = paper[\"title\"], paper[\"content\"]\n",
    "        query = \"\\n\\n\".join([\n",
    "                                f\"Please summarize in extreme detail the following paper titled \\\"{title}\\\" with a focus on how this paper relates to \\\"{topic}\\\".\",\n",
    "                            ] + content)\n",
    "\n",
    "        # Run query to get summary\n",
    "        thread = client.beta.threads.create()\n",
    "        client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=query)\n",
    "        run = client.beta.threads.runs.create_and_poll(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id,\n",
    "            model=model,\n",
    "            temperature=CONSISTENT_TEMPERATURE,\n",
    "        )\n",
    "        messages = get_messages(client=client, thread_id=thread.id)\n",
    "        summary = messages[-1][\"content\"][0][\"text\"][\"value\"]\n",
    "\n",
    "        # Save summary\n",
    "        with open(topic_summary_dir / f\"{pmcid}.json\", \"w\") as f:\n",
    "            json.dump({\"title\": title, \"summary\": summary}, f, indent=4, sort_keys=True)"
   ],
   "id": "58ad321787cb7e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convert summaries to training data format\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    # Load title to PMC ID dictionary\n",
    "    with open(finetuning_dir / f\"{topic_name}_title_to_pmcid.json\") as f:\n",
    "        title_to_pmcid: dict[str, str] = json.load(f)\n",
    "\n",
    "    # Convert summaries to training data format\n",
    "    training_data = []\n",
    "\n",
    "    for title, pmcid in title_to_pmcid.items():\n",
    "        # Load paper summary\n",
    "        with open(summaries_dir / topic_name / f\"{pmcid}.txt\") as f:\n",
    "            summary = f.read()\n",
    "\n",
    "        # Add example to training data\n",
    "        training_data.append(\n",
    "            {\"messages\": [{\"role\": \"system\", \"content\": agent.prompt},\n",
    "                          {\"role\": \"user\", \"content\": \"Tell me about the paper \\\"{title}\\\".\"},\n",
    "                          {\"role\": \"assistant\", \"content\": summary}]}\n",
    "        )\n",
    "\n",
    "    print(f\"Number of paper examples for {topic}: {len(training_data):,}\")\n",
    "\n",
    "    # TODO: count tokens\n",
    "    # TODO: estimate fine-tuning pricing\n",
    "\n",
    "    # Save training data in jsonl format\n",
    "    with open(finetuning_dir / f\"{topic_name}_training_data.jsonl\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(json.dumps(example) for example in training_data))"
   ],
   "id": "9ab5352bf603e433",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Upload fine-tuning data\n",
    "topic_to_id = {}\n",
    "\n",
    "for topic in topic_to_agent:\n",
    "    path = finetuning_dir / f\"{topic.replace(' ', '_')}_training_data.jsonl\"\n",
    "\n",
    "    file_object = client.files.create(\n",
    "        file=open(path, \"rb\"),\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "\n",
    "    topic_to_id[topic] = file_object.id\n",
    "\n",
    "# Save topic file IDs\n",
    "with open(finetuning_dir / \"topic_to_id.json\", \"w\") as f:\n",
    "    json.dump(topic_to_id, f)"
   ],
   "id": "ee5c062a1cd23206",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load topic file IDs\n",
    "with open(finetuning_dir / \"topic_to_id.json\") as f:\n",
    "    topic_to_id = json.load(f)\n",
    "\n",
    "topics = sorted(topic_to_id)"
   ],
   "id": "bb0e6da655cb759a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Launch fine-tuning jobs\n",
    "# for topic, file_id in topic_to_id.items():\n",
    "#     client.fine_tuning.jobs.create(\n",
    "#         training_file=file_id,\n",
    "#         model=finetuning_model,\n",
    "#     )\n",
    "\n",
    "topic = topics[5]\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "    training_file=topic_to_id[topic],\n",
    "    model=finetuning_base_model,  # TODO: swap for GPT-4o, not mini\n",
    "    suffix=topic.replace(\" \", \"_\"),\n",
    ")"
   ],
   "id": "7da9369b7c17cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(list(client.fine_tuning.jobs.list())[1].fine_tuned_model)",
   "id": "66ae2d0e00cb8e3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set up topic to fine-tuned model mapping\n",
    "topic_to_model = {\n",
    "    \"AlphaFold-Multimer\": \"ft:gpt-4o-mini-2024-07-18:personal:alphafold-multimer:AmtqgHON\",\n",
    "    \"ESM\": \"ft:gpt-4o-mini-2024-07-18:personal:esm:AmtjuDox\",\n",
    "    \"Rosetta\": \"ft:gpt-4o-mini-2024-07-18:personal:rosetta:Amtuos8C\",\n",
    "    \"SARS-CoV-2 spike protein\": \"ft:gpt-4o-mini-2024-07-18:personal:sars-cov-2-spike-protein:AmuRh1c1\",\n",
    "    \"SARS-CoV-2 variants KP.3 and JN.1\": \"ft:gpt-4o-mini-2024-07-18:personal:sars-cov-2-variants-kp-3-and-jn-1:AmyELRn8\",\n",
    "    \"nanobodies\": \"ft:gpt-4o-mini-2024-07-18:personal:nanobodies:AmyVyYww\",\n",
    "}"
   ],
   "id": "295b1b032e55cc98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "topic = \"SARS-CoV-2 variants KP.3 and JN.1\"\n",
    "agent = topic_to_agent[topic]\n",
    "query = \"How are the JN.1 and KP.3 variants of SARS-CoV-2 related to each other?\"\n",
    "\n",
    "for selected_model, selected_model_name in [(finetuning_base_model, \"base model\"),\n",
    "                                            (topic_to_model[topic], \"fine-tuned model\")]:\n",
    "    print(f\"Running query \\\"{query}\\\" with {selected_model_name} for {topic}.\\n\")\n",
    "\n",
    "    assistant = client.beta.assistants.create(name=agent.title, instructions=agent.prompt, model=selected_model)\n",
    "    thread = client.beta.threads.create()\n",
    "    client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=query)\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        model=selected_model,\n",
    "        temperature=CONSISTENT_TEMPERATURE,\n",
    "    )\n",
    "\n",
    "    print(run.status)\n",
    "\n",
    "    # if not run.status == \"complete\":\n",
    "    #     print(\"Query failed to complete.\")\n",
    "    #     continue\n",
    "\n",
    "    messages = get_messages(client=client, thread_id=thread.id)\n",
    "\n",
    "    print(messages[-1][\"content\"][0][\"text\"][\"value\"])\n",
    "    print()"
   ],
   "id": "51a0bdefdd05dd63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "77b7c3f7cde6f733",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
