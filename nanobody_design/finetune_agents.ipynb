{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports",
   "id": "e0b884adf3d4f516"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import concurrent.futures\n",
    "import json\n",
    "import re\n",
    "from random import Random\n",
    "\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "from tqdm import tqdm\n",
    "\n",
    "from virtual_lab.agent import Agent\n",
    "from virtual_lab.constants import CONSISTENT_TEMPERATURE, DEFAULT_FINETUNING_EPOCHS\n",
    "from virtual_lab.run_meeting import run_meeting\n",
    "from virtual_lab.utils import async_get_messages, compute_finetuning_cost, compute_token_cost, count_tokens, \\\n",
    "    get_messages, get_pubmed_central_article\n",
    "\n",
    "from nanobody_constants import (\n",
    "    background_prompt,\n",
    "    nanobody_prompt,\n",
    "    discussions_phase_to_dir,\n",
    "    model as base_model,\n",
    "    finetuning_base_model,\n",
    "    immunologist,\n",
    "    machine_learning_specialist,\n",
    "    computational_biologist,\n",
    ")"
   ],
   "id": "a0472e92df8ee037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup",
   "id": "df3e8b02a3f2bb9e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Constants\n",
    "finetuning_dir = discussions_phase_to_dir[\"finetuning\"]\n",
    "papers_dir = finetuning_dir / \"papers\"\n",
    "summaries_dir = finetuning_dir / \"summaries\"\n",
    "qa_dir = finetuning_dir / \"qa_pairs\"\n",
    "\n",
    "for dir_path in [finetuning_dir, papers_dir, summaries_dir, qa_dir]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "client = OpenAI()\n",
    "async_client = AsyncOpenAI()\n",
    "num_concurrent = 10\n",
    "max_tokens = 250000"
   ],
   "id": "fc28ade3791ee419",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Topic to agent mapping\n",
    "topic_to_agent = {\n",
    "    \"nanobodies\": immunologist,\n",
    "    \"SARS-CoV-2 spike protein\": immunologist,\n",
    "    \"SARS-CoV-2 variants KP.3 and JN.1\": immunologist,\n",
    "    \"ESM\": machine_learning_specialist,\n",
    "    \"AlphaFold-Multimer\": computational_biologist,\n",
    "    \"Rosetta\": computational_biologist,\n",
    "}"
   ],
   "id": "164ca17b6c09fc08",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%autoawait asyncio",
   "id": "97d6c25e366d62a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "async def run_query(semaphore: asyncio.Semaphore, agent: Agent, query: str, model: str = base_model) -> str:\n",
    "    \"\"\"Run a query using the model.\n",
    "\n",
    "    :param semaphore: Semaphore to limit the number of concurrent requests.\n",
    "    :param agent: Agent to use for the query.\n",
    "    :param query: Query to run.\n",
    "    :param model: Model to use for the query.\n",
    "    :return: Response from the model.\n",
    "    \"\"\"\n",
    "    async with semaphore:\n",
    "        assistant = await async_client.beta.assistants.create(name=agent.title, instructions=agent.prompt, model=model)\n",
    "        thread = await async_client.beta.threads.create()\n",
    "        await async_client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=query)\n",
    "        await async_client.beta.threads.runs.create_and_poll(\n",
    "            thread_id=thread.id,\n",
    "            assistant_id=assistant.id,\n",
    "            model=model,\n",
    "            temperature=CONSISTENT_TEMPERATURE,\n",
    "        )\n",
    "        messages = await async_get_messages(client=async_client, thread_id=thread.id)\n",
    "        response = messages[-1][\"content\"][0][\"text\"][\"value\"]\n",
    "\n",
    "    return response"
   ],
   "id": "265f6546d2ec031",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PubMed Central search queries",
   "id": "59f5e2f3a5448780"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use the agents to generate PubMed Central search queries by topic.",
   "id": "fb7a07c38d6579a8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    concurrent.futures.wait([\n",
    "        executor.submit(\n",
    "            run_meeting,\n",
    "            meeting_type=\"individual\",\n",
    "            team_member=agent,\n",
    "            agenda=f\"{background_prompt} {nanobody_prompt} You are responsible for understanding the topic \\\"{topic}\\\" in the context of designing nanobody binders for SARS-CoV-2. You need to fine-tune yourself on the relevant literature on {topic} to improve your ability to design SARS-CoV-2 nanobody binders. Please write out a series of five distinct search queries that you want to run to find relevant scientific papers on {topic}. Include both queries about {topic} generally as well as queries about how {topic} relates to designing nanobody binders for SARS-CoV-2. Please provide the queries in Python syntax as a list of double-quoted strings.\",\n",
    "            agenda_questions=(\n",
    "                f\"What are the queries that you want to perform to identify the relevant literature on {topic} (as a list of double-quoted strings in Python syntax)?\",),\n",
    "            save_dir=finetuning_dir,\n",
    "            save_name=f\"{topic.replace(' ', '_')}_queries\",\n",
    "            temperature=CONSISTENT_TEMPERATURE,\n",
    "            model=base_model,\n",
    "        ) for topic, agent in topic_to_agent.items()\n",
    "    ])"
   ],
   "id": "2ac338d6d173bf0e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Extract the search queries from the agent responses.",
   "id": "4982aa5b11f7381e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set up regex pattern for extracting queries\n",
    "query_list_pattern = re.compile(r'\\[\\s*(\".*?\"\\s*(,\\s*\".*?\"\\s*)*)?,?\\s*\\]')\n",
    "\n",
    "topic_to_queries = {}\n",
    "\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    # Get query path for topic\n",
    "    query_path = finetuning_dir / f\"{topic.replace(' ', '_')}_queries.json\"\n",
    "\n",
    "    # Load query discussion\n",
    "    with open(query_path) as f:\n",
    "        query_discussion = json.load(f)\n",
    "\n",
    "    # Extract queries\n",
    "    query_message = query_discussion[-1][\"message\"]\n",
    "    pattern_result = query_list_pattern.search(query_message)\n",
    "\n",
    "    # Check if pattern is matched\n",
    "    if pattern_result is None:\n",
    "        print(f\"No queries found for {query_path}\")\n",
    "        continue\n",
    "\n",
    "    # Extract queries\n",
    "    queries = json.loads(pattern_result.group())\n",
    "    topic_to_queries[topic] = queries"
   ],
   "id": "603675b18435369e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## PubMed Central papers",
   "id": "c3efba9cdf0a6f0c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Have the agents find papers on PubMed Central using the search queries (100 papers per query) and evaluate them based on their abstracts.",
   "id": "ca92296db243ce0c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for topic, agent in topic_to_agent.items():\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        concurrent.futures.wait([\n",
    "            executor.submit(\n",
    "                run_meeting,\n",
    "                meeting_type=\"individual\",\n",
    "                team_member=agent,\n",
    "                agenda=f\"{background_prompt} {nanobody_prompt} You are responsible for understanding the topic \\\"{topic}\\\" in the context of designing nanobody binders for SARS-CoV-2. You need to fine-tune yourself on the relevant literature on {topic} to improve your ability to design SARS-CoV-2 nanobody binders. Please use PubMed Central and search for relevant papers on {topic} using the query \\\"{query}\\\" and request 100 articles with abstracts only. Read all of the abstracts and based on each abstract individually, decide whether you want to fine-tune yourself on the full text of that paper. Include as many papers as possible, but only include papers that are directly relevant to {topic}. Please provide the PMCIDs and titles of all the papers that you wish to fine-tune yourself on as a Python dictionary mapping PMCID as a double-quoted string to title as a double-quoted string.\",\n",
    "                agenda_questions=(\n",
    "                    \"What are the PMCIDs and titles of the papers you wish to fine-tune yourself on (as a Python dictionary mapping PMCID as a double-quoted string to title as double-quoted string)?\",),\n",
    "                save_dir=finetuning_dir,\n",
    "                save_name=f\"{topic.replace(' ', '_')}_papers_{query_num + 1}\",\n",
    "                temperature=CONSISTENT_TEMPERATURE,\n",
    "                model=base_model,\n",
    "                pubmed_search=True,\n",
    "            ) for query_num, query in enumerate(topic_to_queries[topic]) if not (discussions_phase_to_dir[\n",
    "                                                                                     \"finetuning\"] / f\"{topic.replace(' ', '_')}_papers_{query_num + 1}.json\").exists()\n",
    "        ])"
   ],
   "id": "2ba3181c77e2e7b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Extract the selected papers from the agent responses.",
   "id": "47eff2b4e981b180"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Set up regex pattern for extracting queries\n",
    "pmcid_to_title_pattern = re.compile(r'\\{\\s*(\".*?\"\\s*:\\s*\".*?\"\\s*(,\\s*\".*?\"\\s*:\\s*\".*?\"\\s*)*)?\\}')\n",
    "\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    # Set up title to PMC ID dictionary\n",
    "    title_to_pmcid = {}\n",
    "    titles_lower, pmcids = set(), set()\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    # Get all paper paths for a topic\n",
    "    paper_paths = sorted(finetuning_dir.glob(f\"{topic_name}_papers_*.json\"))\n",
    "\n",
    "    # Check if all papers results are present\n",
    "    if len(paper_paths) != 5:\n",
    "        print(f\"Missing papers for {topic}\")\n",
    "        continue\n",
    "\n",
    "    # Extract PMC IDs and titles from each papers file\n",
    "    for paper_path in paper_paths:\n",
    "        # Load paper discussion\n",
    "        with open(paper_path) as f:\n",
    "            paper_discussion = json.load(f)\n",
    "\n",
    "        # Extract PMC IDs and titles dictionary\n",
    "        paper_message = paper_discussion[1][\"message\"]\n",
    "        pattern_result = pmcid_to_title_pattern.search(paper_message)\n",
    "\n",
    "        # Check if pattern is matched\n",
    "        if pattern_result is None:\n",
    "            print(f\"No papers found for {paper_path}\")\n",
    "            continue\n",
    "\n",
    "        # Extract PMC IDs and titles dictionary\n",
    "        pmcid_to_title = json.loads(pattern_result.group())\n",
    "\n",
    "        # Add PMC IDs and titles to dictionary, avoiding duplicates\n",
    "        for pmcid, title in pmcid_to_title.items():\n",
    "            # Replace en dash and em dash with a hyphen and convert to lowercase\n",
    "            title = title.replace(\"–\", \"-\").replace(\"—\", \"-\")\n",
    "            title_lower = title.lower()\n",
    "\n",
    "            if title_lower not in titles_lower and pmcid not in pmcids:\n",
    "                title_to_pmcid[title] = pmcid\n",
    "                titles_lower.add(title_lower)\n",
    "                pmcids.add(pmcid)\n",
    "\n",
    "    print(f\"Number of papers found for {topic}: {len(title_to_pmcid):,}\")\n",
    "\n",
    "    # Save title to PMC ID dictionary\n",
    "    with open(finetuning_dir / f\"{topic_name}_title_to_pmcid.json\", \"w\") as f:\n",
    "        json.dump(title_to_pmcid, f, indent=4, sort_keys=True)"
   ],
   "id": "b94895349ed6bb8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the PMCIDs and titles from the agent responses.",
   "id": "e54a7fd55abb9a8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pmcids = set()\n",
    "\n",
    "for topic in topic_to_agent:\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    with open(finetuning_dir / f\"{topic_name}_title_to_pmcid.json\") as f:\n",
    "        title_to_pmcid = json.load(f)\n",
    "\n",
    "    pmcids.update(title_to_pmcid.values())\n",
    "\n",
    "print(f\"Number of unique PMCIDs: {len(pmcids):,}\")"
   ],
   "id": "722a42e4e434064d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Download the papers from PubMed Central.",
   "id": "9749cf90c23f4a08"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "paper_count = 0\n",
    "\n",
    "for pmcid in tqdm(sorted(pmcids)):\n",
    "    title, content = get_pubmed_central_article(pmcid=pmcid)\n",
    "\n",
    "    if title is None:\n",
    "        continue\n",
    "\n",
    "    paper_count += 1\n",
    "\n",
    "    # Save paper\n",
    "    with open(papers_dir / f\"{pmcid}.json\", \"w\") as f:\n",
    "        json.dump({\"title\": title, \"content\": content}, f, indent=4, sort_keys=True)\n",
    "\n",
    "print(f\"Number of papers downloaded: {paper_count:,}\")"
   ],
   "id": "75555d31fe196365",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Summarize papers",
   "id": "ddc033fce2bae98d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Define a function to using an agent to summarize a paper.",
   "id": "40ea1abc921423db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "async def summarize_paper(semaphore: asyncio.Semaphore, agent: Agent, topic: str, pmcid: str, title: str,\n",
    "                          content: list[str]) -> tuple[str, str, str]:\n",
    "    \"\"\"Summarize a paper using the model.\n",
    "\n",
    "    :param semaphore: Semaphore to limit the number of concurrent requests.\n",
    "    :param agent: Agent to use for summarization.\n",
    "    :param topic: Topic of interest.\n",
    "    :param pmcid: PMC ID of the paper.\n",
    "    :param title: Title of the paper.\n",
    "    :param content: Content of the paper.\n",
    "    :return: Tuple of PMC ID, title, and summary of the paper.\n",
    "    \"\"\"\n",
    "    # Set up query with paper\n",
    "    query = \"\\n\\n\".join([\n",
    "                            f\"Please summarize in extreme detail the following paper titled \\\"{title}\\\". Please focus in particular on summarizing key insights about the topic \\\"{topic}\\\" in relation to designing SARS-CoV-2 nanobody binders.\"] + content)\n",
    "\n",
    "    # Run query to get summary\n",
    "    summary = await run_query(\n",
    "        semaphore=semaphore,\n",
    "        agent=agent,\n",
    "        query=query,\n",
    "    )\n",
    "\n",
    "    return pmcid, title, summary"
   ],
   "id": "5b0c48ff62aaf328",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Use the agents to summarize each paper in parallel.",
   "id": "8cc6e8820de27df6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for topic, agent in topic_to_agent.items():\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    # Create save directory\n",
    "    topic_summary_dir = summaries_dir / topic_name\n",
    "    topic_summary_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load title to PMC ID dictionary\n",
    "    with open(finetuning_dir / f\"{topic_name}_title_to_pmcid.json\") as f:\n",
    "        title_to_pmcid: dict[str, str] = json.load(f)\n",
    "\n",
    "    # Get unique PMC IDs\n",
    "    pmcids = sorted(set((title_to_pmcid.values())))\n",
    "\n",
    "    # Load papers\n",
    "    pmcid_to_paper = {}\n",
    "    for pmcid in pmcids:\n",
    "        paper_path = papers_dir / f\"{pmcid}.json\"\n",
    "\n",
    "        if paper_path.exists():\n",
    "            with open(paper_path) as f:\n",
    "                paper: dict[str, str | list[str]] = json.load(f)\n",
    "                pmcid_to_paper[pmcid] = paper\n",
    "\n",
    "    print(f\"Number of papers loaded for {topic}: {len(pmcid_to_paper):,}\")\n",
    "\n",
    "    # Limit papers by length\n",
    "    pmcid_to_paper = {pmcid: paper for pmcid, paper in pmcid_to_paper.items() if\n",
    "                      sum(count_tokens(paragraph) for paragraph in paper[\"content\"]) <= max_tokens}\n",
    "\n",
    "    print(f\"Number of papers after token limit of {max_tokens:,} for {topic}: {len(pmcid_to_paper):,}\")\n",
    "\n",
    "    # Compute input token cost\n",
    "    input_token_count = sum(count_tokens(paragraph) for paper in pmcid_to_paper.values() for paragraph in\n",
    "                            [paper[\"title\"]] + paper[\"content\"])\n",
    "\n",
    "    input_token_cost = compute_token_cost(\n",
    "        model=base_model,\n",
    "        input_token_count=input_token_count,\n",
    "        output_token_count=0,\n",
    "    )\n",
    "\n",
    "    print(f\"Approximate input token cost for {topic} = ${input_token_cost:.2f}\")\n",
    "\n",
    "    # Set up semaphore with the number of concurrent requests\n",
    "    semaphore = asyncio.Semaphore(num_concurrent)\n",
    "\n",
    "    # Create tasks for each paper\n",
    "    tasks = [\n",
    "        asyncio.create_task(\n",
    "            summarize_paper(semaphore=semaphore, agent=agent, topic=topic, pmcid=pmcid, title=paper[\"title\"],\n",
    "                            content=paper[\"content\"]))\n",
    "        for pmcid, paper in pmcid_to_paper.items()\n",
    "    ]\n",
    "\n",
    "    # Run agent summary for each paper\n",
    "    results = [(await task) for task in tqdm(asyncio.as_completed(tasks), total=len(tasks))]\n",
    "\n",
    "    # Save summaries\n",
    "    for pmcid, title, summary in results:\n",
    "        with open(topic_summary_dir / f\"{pmcid}.json\", \"w\") as f:\n",
    "            json.dump({\"pmcid\": pmcid, \"title\": title, \"summary\": summary}, f, indent=4, sort_keys=True)"
   ],
   "id": "58ad321787cb7e8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Convert the summaries to the format required for fine-tuning.",
   "id": "e139b03bfed05c9d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for topic, agent in topic_to_agent.items():\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    # Get summary paths\n",
    "    topic_summary_dir = summaries_dir / topic_name\n",
    "    summary_paths = sorted(topic_summary_dir.glob(\"*.json\"))\n",
    "\n",
    "    # Convert summaries to training data format\n",
    "    training_data = []\n",
    "\n",
    "    for summary_path in summary_paths:\n",
    "        # Load paper summary data\n",
    "        with open(summary_path) as f:\n",
    "            summary_data = json.load(f)\n",
    "\n",
    "        # Extract title and summary\n",
    "        title, summary = summary_data[\"title\"], summary_data[\"summary\"]\n",
    "\n",
    "        # Add example to training data\n",
    "        training_data.append(\n",
    "            {\"messages\": [{\"role\": \"system\", \"content\": agent.prompt},\n",
    "                          {\"role\": \"user\",\n",
    "                           \"content\": f\"Please tell me about the paper \\\"{title}\\\" and its insights into \\\"{topic}\\\" in relation to designing SARS-CoV-2 nanobody binders.\"},\n",
    "                          {\"role\": \"assistant\", \"content\": summary}]}\n",
    "        )\n",
    "\n",
    "    # Count tokens\n",
    "    token_count = sum(count_tokens(message[\"content\"]) for data in training_data for message in data[\"messages\"])\n",
    "\n",
    "    # Determine finetuning cost\n",
    "    finetuning_cost = compute_finetuning_cost(\n",
    "        model=finetuning_base_model,\n",
    "        token_count=token_count,\n",
    "        num_epochs=DEFAULT_FINETUNING_EPOCHS,\n",
    "    )\n",
    "\n",
    "    # Print stats\n",
    "    print(f\"Number of paper examples for {topic}: {len(training_data):,}\")\n",
    "    print(f\"Token count for {topic}: {token_count:,}\")\n",
    "    print(f\"Finetuning cost for {topic}: ${finetuning_cost:.2f}\")\n",
    "    print()\n",
    "\n",
    "    # Save training data in jsonl format\n",
    "    with open(finetuning_dir / f\"{topic_name}_training_data.jsonl\", \"w\") as f:\n",
    "        f.write(\"\\n\".join(json.dumps(example) for example in training_data))"
   ],
   "id": "9ab5352bf603e433",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Fine-tuning",
   "id": "2b57b1c784cecb01"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Upload summaries as fine-tuning data and save a mapping from topic to data ID.",
   "id": "45f6e0742afda16f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "topic_to_id = {}\n",
    "\n",
    "for topic in topic_to_agent:\n",
    "    path = finetuning_dir / f\"{topic.replace(' ', '_')}_training_data.jsonl\"\n",
    "\n",
    "    file_object = client.files.create(\n",
    "        file=open(path, \"rb\"),\n",
    "        purpose=\"fine-tune\"\n",
    "    )\n",
    "\n",
    "    topic_to_id[topic] = file_object.id\n",
    "\n",
    "with open(finetuning_dir / \"topic_to_id.json\", \"w\") as f:\n",
    "    json.dump(topic_to_id, f)"
   ],
   "id": "ee5c062a1cd23206",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load mapping from topic to data ID.",
   "id": "b62f6f0d2c26b7f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(finetuning_dir / \"topic_to_id.json\") as f:\n",
    "    topic_to_id = json.load(f)\n",
    "\n",
    "topics = sorted(topic_to_id)"
   ],
   "id": "bb0e6da655cb759a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Launch fine-tuning jobs.",
   "id": "32f052905ad0ce88"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# for topic, file_id in topic_to_id.items():\n",
    "#     client.fine_tuning.jobs.create(\n",
    "#         training_file=file_id,\n",
    "#         model=finetuning_model,\n",
    "#     )\n",
    "\n",
    "topic = topics[5]\n",
    "\n",
    "client.fine_tuning.jobs.create(\n",
    "    training_file=topic_to_id[topic],\n",
    "    model=finetuning_base_model,  # TODO: swap for GPT-4o, not mini\n",
    "    suffix=topic.replace(\" \", \"_\"),\n",
    ")"
   ],
   "id": "7da9369b7c17cd2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Check fine-tuning job status.",
   "id": "3704a4f311328397"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(list(client.fine_tuning.jobs.list())[0].status)",
   "id": "66ae2d0e00cb8e3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Set up mapping from topic to fine-tuned model ID.",
   "id": "dfd9378d12584190"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "topic_to_model = {\n",
    "    \"AlphaFold-Multimer\": \"ft:gpt-4o-mini-2024-07-18:personal:alphafold-multimer:ArGNLO1a\",\n",
    "    \"ESM\": \"ft:gpt-4o-mini-2024-07-18:personal:esm:ArFrZ4tt\",\n",
    "    \"Rosetta\": \"ft:gpt-4o-mini-2024-07-18:personal:rosetta:ArFx2nis\",\n",
    "    \"SARS-CoV-2 spike protein\": \"ft:gpt-4o-mini-2024-07-18:personal:sars-cov-2-spike-protein:ArG9fZwz\",\n",
    "    \"SARS-CoV-2 variants KP.3 and JN.1\": \"ft:gpt-4o-mini-2024-07-18:personal:sars-cov-2-variants-kp-3-and-jn-1:ArGVyHfW\",\n",
    "    \"nanobodies\": \"ft:gpt-4o-mini-2024-07-18:personal:nanobodies:ArGjRZ9R\",\n",
    "}"
   ],
   "id": "295b1b032e55cc98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluate",
   "id": "b8a8303b0aafe48c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create a set of question and answer pairs to evaluate the fine-tuned models based on the summaries.",
   "id": "2b60fd2c0b23933"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "async def generate_qa_pairs(semaphore: asyncio.Semaphore, agent: Agent, pmcid: str, summary: str) -> tuple[str, str]:\n",
    "    \"\"\"Generate question and answer pairs based on the summary using the model.\n",
    "\n",
    "    :param semaphore: Semaphore to limit the number of concurrent requests.\n",
    "    :param agent: Agent to use for summarization.\n",
    "    :param pmcid: PMC ID of the paper.\n",
    "    :param summary: Summary of the paper.\n",
    "    :return: Tuple of PMC ID and response (string of question and answer pairs in JSON dictionary form).\n",
    "    \"\"\"\n",
    "    # Set up query with summary\n",
    "    query = f\"Please generate five unique questions based on the following summary of a scientific paper. The questions should be designed so that they are specific to the summary rather than general knowledge, i.e., they can only be answered with information in the summary. However, the questions must include enough context so that they can be answered without knowing which specific paper is being referred to (i.e., do not say things like \\\"in this study\\\"). For the answers, please write one correct answer and three incorrect answers for each question. Make the incorrect answers as plausible as possible so that selecting the correct answer is challenging without specific knowledge of the paper summary. Your response should *only* be a JSON dictionary mapping from question (string) to answers (list of four strings where the first string is the correct answer and the other three are incorrect). Here is the summary:\\n\\n{summary}\"\n",
    "\n",
    "    # Run query to get questions and answers\n",
    "    response = await run_query(\n",
    "        semaphore=semaphore,\n",
    "        agent=agent,\n",
    "        query=query,\n",
    "    )\n",
    "\n",
    "    return pmcid, response"
   ],
   "id": "6dc6497c4af7946e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for topic, agent in topic_to_agent.items():\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    # Create save directory\n",
    "    topic_qa_dir = qa_dir / topic_name\n",
    "    topic_qa_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Get summary paths for the topic\n",
    "    summary_paths = sorted((summaries_dir / topic_name).glob(\"*.json\"))\n",
    "\n",
    "    # Load PMCID to summary mapping\n",
    "    pmcid_to_summary = {}\n",
    "    for summary_path in summary_paths:\n",
    "        # Load paper summary data\n",
    "        with open(summary_path) as f:\n",
    "            summary_data = json.load(f)\n",
    "\n",
    "        # Extract PMCID and summary\n",
    "        pmcid, summary = summary_data[\"pmcid\"], summary_data[\"summary\"]\n",
    "\n",
    "        # Add PMCID to summary mapping\n",
    "        pmcid_to_summary[pmcid] = summary\n",
    "\n",
    "    print(f\"Number of summaries loaded for {topic}: {len(pmcid_to_summary):,}\")\n",
    "\n",
    "    # Compute input token cost\n",
    "    input_token_count = sum(count_tokens(summary) for summary in pmcid_to_summary.values())\n",
    "\n",
    "    input_token_cost = compute_token_cost(\n",
    "        model=base_model,\n",
    "        input_token_count=input_token_count,\n",
    "        output_token_count=0,\n",
    "    )\n",
    "\n",
    "    print(f\"Approximate input token cost for {topic} = ${input_token_cost:.2f}\")\n",
    "\n",
    "    # Set up semaphore with the number of concurrent requests\n",
    "    semaphore = asyncio.Semaphore(num_concurrent)\n",
    "\n",
    "    # Create tasks for each summary\n",
    "    tasks = [\n",
    "        asyncio.create_task(generate_qa_pairs(semaphore=semaphore, agent=agent, pmcid=pmcid, summary=summary))\n",
    "        for pmcid, summary in pmcid_to_summary.items()\n",
    "    ]\n",
    "\n",
    "    # Run agent questions for each paper\n",
    "    results = [(await task) for task in tqdm(asyncio.as_completed(tasks), total=len(tasks))]\n",
    "\n",
    "    # Parse JSON Q&A pairs\n",
    "    pmcid_to_qa_pairs = {}\n",
    "    for pmcid, qa_pairs in results:\n",
    "        try:\n",
    "            pmcid_to_qa_pairs[pmcid] = json.loads(qa_pairs.replace(\"```json\", \"\").replace(\"```\", \"\").strip())\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Failed to parse JSON for {pmcid}\")\n",
    "            print(qa_pairs)\n",
    "\n",
    "    # Save summaries\n",
    "    for pmcid, qa_pairs in pmcid_to_qa_pairs.items():\n",
    "        with open(topic_qa_dir / f\"{pmcid}.json\", \"w\") as f:\n",
    "            json.dump({\"pmcid\": pmcid, \"qa_pairs\": qa_pairs}, f, indent=4, sort_keys=True)"
   ],
   "id": "e9a4d835b5e5b242",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Function to use an agent to answer a question.",
   "id": "3ae60be5d078daa0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "async def answer_question(semaphore: asyncio.Semaphore, agent: Agent, question: str, answers: list[str], model: str) -> \\\n",
    "tuple[\n",
    "    str, str]:\n",
    "    \"\"\"Answer a question using the model.\n",
    "\n",
    "    :param semaphore: Semaphore to limit the number of concurrent requests.\n",
    "    :param agent: Agent to use for answering the question.\n",
    "    :param question: Question to ask.\n",
    "    :param answers: List of answers to choose from in random order.\n",
    "    :param model: Model to use for answering the question.\n",
    "    :return: Tuple of question and selected answer.\n",
    "    \"\"\"\n",
    "    # Set up query with question and answer options\n",
    "    query = f\"Please answer the following question based on the following list of answers. Only one answer is correct. Please select the correct answer and provide as your response the exact text of the answer and nothing else. Here is the question:\\n\\n{question}\\n\\nHere are the possible answers (choose the correct one):\\n\\n{'\\n\\n'.join(answers)}\"\n",
    "\n",
    "    # Run query to get answer\n",
    "    answer = await run_query(\n",
    "        semaphore=semaphore,\n",
    "        agent=agent,\n",
    "        query=query,\n",
    "        model=model,\n",
    "    )\n",
    "\n",
    "    return question, answer"
   ],
   "id": "bec74be5de5f2464",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Evaluate the fine-tuned models on question and answer pairs.",
   "id": "20894f925bb4a125"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "max_num_qa_per_topic = 100\n",
    "\n",
    "for topic, agent in topic_to_agent.items():\n",
    "    topic_name = topic.replace(' ', '_')\n",
    "\n",
    "    # Get Q&A paths for the topic\n",
    "    qa_paths = sorted((qa_dir / topic_name).glob(\"*.json\"))\n",
    "\n",
    "    # Load Q&A pairs\n",
    "    qa_pairs = {}\n",
    "    for qa_path in qa_paths:\n",
    "        # Load paper summary data\n",
    "        with open(qa_path) as f:\n",
    "            qa_data = json.load(f)\n",
    "\n",
    "        # Add Q&A pairs\n",
    "        qa_pairs |= qa_data[\"qa_pairs\"]\n",
    "\n",
    "    print(f\"Number of Q&A pairs loaded for {topic}: {len(qa_pairs):,}\")\n",
    "\n",
    "    # Set up randomness\n",
    "    random = Random(0)\n",
    "\n",
    "    # Limit number of Q&A pairs with random sampling\n",
    "    if len(qa_pairs) > max_num_qa_per_topic:\n",
    "        qa_pairs_list = list(qa_pairs.items())\n",
    "        random.shuffle(qa_pairs_list)\n",
    "        qa_pairs = dict(qa_pairs_list[:max_num_qa_per_topic])\n",
    "        print(f\"Number of Q&A pairs after random sampling for {topic}: {len(qa_pairs):,}\")\n",
    "\n",
    "    # Map questions to correct answers\n",
    "    question_to_correct_answer = {question: answers[0] for question, answers in qa_pairs.items()}\n",
    "\n",
    "    # Randomize order of answers\n",
    "    for answers in qa_pairs.values():\n",
    "        random.shuffle(answers)\n",
    "\n",
    "    # Loop over models\n",
    "    for model, model_name in [(finetuning_base_model, \"base model\"), (topic_to_model[topic], \"fine-tuned model\")]:\n",
    "        print(f\"Evaluating {model_name} for {topic}.\")\n",
    "\n",
    "        # Set up semaphore with the number of concurrent requests\n",
    "        semaphore = asyncio.Semaphore(num_concurrent)\n",
    "\n",
    "        # Create tasks for each question\n",
    "        tasks = [\n",
    "            asyncio.create_task(\n",
    "                answer_question(semaphore=semaphore, agent=agent, question=question, answers=answers, model=model))\n",
    "            for question, answers in qa_pairs.items()\n",
    "        ]\n",
    "\n",
    "        # Run agent answers for each paper\n",
    "        results = [(await task) for task in tqdm(asyncio.as_completed(tasks), total=len(tasks))]\n",
    "\n",
    "        # Map questions to selected answers\n",
    "        question_to_selected_answer = {question: answer for question, answer in results}\n",
    "\n",
    "        # Compute accuracy\n",
    "        accuracy = sum(question_to_correct_answer[question] == question_to_selected_answer[question] for question in\n",
    "                       question_to_correct_answer) / len(question_to_correct_answer)\n",
    "\n",
    "        print(f\"Accuracy: {accuracy:.2%}\")"
   ],
   "id": "a8f8f524c27aa960",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "topic = \"SARS-CoV-2 variants KP.3 and JN.1\"\n",
    "agent = topic_to_agent[topic]\n",
    "query = \"What mutations are present in the KP.3 and JN.1 variants of SARS-CoV-2?\"\n",
    "\n",
    "for selected_model, selected_model_name in [(finetuning_base_model, \"base model\"),\n",
    "                                            (topic_to_model[topic], \"fine-tuned model\")]:\n",
    "    print(f\"Running query \\\"{query}\\\" with {selected_model_name} for {topic}.\\n\")\n",
    "\n",
    "    assistant = client.beta.assistants.create(name=agent.title, instructions=agent.prompt, model=selected_model)\n",
    "    thread = client.beta.threads.create()\n",
    "    client.beta.threads.messages.create(thread_id=thread.id, role=\"user\", content=query)\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "        model=selected_model,\n",
    "        temperature=CONSISTENT_TEMPERATURE,\n",
    "    )\n",
    "\n",
    "    messages = get_messages(client=client, thread_id=thread.id)\n",
    "\n",
    "    print(messages[-1][\"content\"][0][\"text\"][\"value\"])\n",
    "    print()"
   ],
   "id": "51a0bdefdd05dd63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "77b7c3f7cde6f733",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 5,
 "nbformat_minor": 9
}
